{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd067b1319a16a8d55d48fc73d6c08eda16d2e612bbaddba2e6ca3170581266de9e",
   "display_name": "Python 3.9.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_ast import process_source, get_ast, get_sbt_structure\n",
    "process_source('source.java', 'lexed.java')\n",
    "get_ast('lexed.java', 'ast.json')\n",
    "get_sbt_structure(\"ast.json\", \"sbt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Tokenizer\n",
    "def read_vocab(input_path, return_data=False):\n",
    "    with open(input_path, 'r') as content_file:\n",
    "        data = content_file.read().split('\\n')\n",
    "    vocab_dict = {k: v for (v, k) in enumerate(data, start=1)}\n",
    "    if return_data:\n",
    "        return vocab_dict, data\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AST_VOCAB = \"../../Dataset/data_RQ1/vocab.ast\"\n",
    "CODE_VOCAB= \"../../Dataset/data_RQ1/vocab.code\"\n",
    "ast_vocab = read_vocab(AST_VOCAB)\n",
    "code_vocab= read_vocab(CODE_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbt_input = \"sbt.json\"\n",
    "code_input= \"sbt.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sbt_input, 'r') as f:\n",
    "    ast = f.read()\n",
    "with open(code_input, 'r') as f:\n",
    "    code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 286]), torch.Size([1, 286]), torch.Size([1, 286]))"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "tast, tcode, code_ex = helper(code, ast)\n",
    "tast.shape, tcode.shape, code_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DeepComEncoder(inp_dim_code=config.vocab_size_code+1,\n",
    "                         inp_dim_ast=config.vocab_size_ast+1).to(device)\n",
    "\n",
    "decoder = AttentionDecoder(inp_dim=config.vocab_size_nl+1).to(device)\n",
    "\n",
    "model_tester = Tester(encoder=encoder,\n",
    "                      decoder=decoder,\n",
    "                      batchqueue=batchqueue,\n",
    "                      batch_sz=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(content, ast):\n",
    "    fill_UNK = lambda lst, vocab: [vocab[\"<UNK>\"] if w >= len(vocab) else w for w in lst]\n",
    "    post_pad = lambda seqs, maxl: [(x + [0]*(maxl-len(x))) for x in seqs]\n",
    "    code_oovs = []\n",
    "    nl_batch = []\n",
    "    ast_batch = []\n",
    "    code_batch = []\n",
    "    ex_code_batch = []\n",
    "    ex_nl_batch = []\n",
    "    l1, l2, l3 = 0, 0, 0\n",
    "    \n",
    "    idxs_ex = []\n",
    "    content = f\"<S> {content} </S>\"\n",
    "    for word in content.split():\n",
    "        try:\n",
    "            idxs_ex.append(code_vocab[word])\n",
    "        except KeyError:\n",
    "            if word not in code_oovs:\n",
    "                code_oovs.append(word)\n",
    "            idxs_ex.append(len(code_vocab) + code_oovs.index(word))\n",
    "    l1 = max(l1, len(idxs_ex))\n",
    "    ex_code_batch.append(idxs_ex)\n",
    "    code_batch.append(fill_UNK(idxs_ex, code_vocab))\n",
    "\n",
    "\n",
    "    ast = f\"<S> {ast} </S>\"\n",
    "    idxs = [ast_vocab[w] for w in ast.split()]\n",
    "    l2 = max(l2, len(idxs))\n",
    "    ast_batch.append(idxs)\n",
    "    \n",
    "    l = max(l1, l2)\n",
    "    ast_batch = post_pad(ast_batch, l)\n",
    "    code_batch = post_pad(code_batch, l)\n",
    "    ex_code_batch = post_pad(ex_code_batch, l)\n",
    "    \n",
    "    ast = torch.Tensor(ast_batch).long().to(device)\n",
    "    code = torch.Tensor(code_batch).long().to(device)\n",
    "    code_ex = torch.Tensor(ex_code_batch).long().to(device)\n",
    "    return ast, code, code_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}