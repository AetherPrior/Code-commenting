{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertLMHeadModel, BertConfig\n",
    "model_name = \"huggingface/CodeBERTa-small-v1\" \n",
    "\n",
    "encoder1 = AutoModel.from_pretrained(model_name)\n",
    "encoder2 = AutoModel.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.is_decoder = True\n",
    "config.add_cross_attention=True\n",
    "config.output_hidden_states=True\n",
    "decoder = BertLMHeadModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "zeros = torch.zeros(1, 1, 768)\n",
    "zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer.encode_plus(\"Hello world!\", return_tensors = \"pt\")\n",
    "output = encoder1(**input)\n",
    "encoder_hidden_state = output[0]\n",
    "decoder_output = decoder(inputs_embeds=zeros, encoder_hidden_states=encoder_hidden_state)\n",
    "decoder_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 30522]), torch.Size([1, 1, 30522]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_gen = torch.sigmoid(decoder_output['logits'])\n",
    "softmax = torch.nn.Softmax(dim=2)\n",
    "p_vocab = softmax(decoder_output['logits'])\n",
    "p_gen.shape, p_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30522])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_gen*p_vocab).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output['hidden_states'][12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_probs \n",
    "context_layer = decoder.bert.encoder.layer[11].attention.self(zeros)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 1, 64])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.5023e-02, -5.3094e-03,  2.3572e-04,  2.0522e-03, -5.7804e-03,\n",
       "            8.1983e-03, -1.1096e-02, -4.4216e-03,  1.0920e-03, -5.7501e-03,\n",
       "           -1.2049e-03,  3.3312e-03, -5.7881e-03,  3.2863e-03,  1.6009e-02,\n",
       "           -3.8654e-03,  7.0702e-03,  9.0583e-03,  9.7348e-03, -8.1410e-03,\n",
       "            1.1158e-02, -5.4021e-03, -1.4808e-02, -6.7811e-03,  5.0396e-03,\n",
       "            4.8309e-03,  1.2264e-02,  1.7033e-03,  3.8622e-03, -5.0164e-03,\n",
       "            1.0777e-02,  2.2679e-03,  1.2602e-02,  1.7358e-03, -8.8981e-03,\n",
       "           -1.6617e-03, -5.4248e-03,  1.5820e-02, -2.7107e-03,  9.3095e-03,\n",
       "            1.4642e-02,  1.8524e-03, -6.2581e-03,  1.3867e-02,  1.6465e-02,\n",
       "           -9.9101e-03, -5.9330e-03,  3.1781e-03,  6.2056e-03, -5.7156e-03,\n",
       "            1.5173e-02,  8.2161e-03, -7.5105e-03, -6.8956e-03,  1.1878e-02,\n",
       "            1.2468e-02, -1.9451e-03,  1.4344e-02,  3.4198e-03,  9.5356e-03,\n",
       "            1.9738e-03, -2.3401e-03,  2.2367e-03, -9.8477e-04,  2.0704e-02,\n",
       "            1.1214e-02,  2.2082e-02, -2.3108e-03, -2.9715e-02,  6.5029e-02,\n",
       "            2.0563e-02, -1.7909e-03,  4.6880e-02,  3.9289e-02, -1.5563e-02,\n",
       "           -1.6345e-02, -1.1725e-02,  8.4864e-03, -2.1302e-03,  1.5644e-03,\n",
       "           -2.6728e-02,  4.6210e-03, -8.3575e-04,  2.1788e-02, -1.8891e-02,\n",
       "            4.3932e-02, -3.4591e-02,  2.0594e-02,  2.4536e-02, -4.4848e-02,\n",
       "           -9.8070e-04, -2.2324e-02, -1.9449e-02,  4.4326e-02, -6.9930e-02,\n",
       "           -2.1216e-02,  1.3391e-02,  2.3917e-02,  2.3278e-02,  7.0228e-03,\n",
       "            4.4435e-02,  1.3210e-02, -5.3480e-02, -4.0530e-02, -3.2905e-02,\n",
       "           -3.2087e-02, -2.6361e-02,  1.2619e-02,  6.9959e-03,  3.2442e-02,\n",
       "           -1.8067e-02, -1.1462e-02, -3.3291e-02, -3.1658e-03, -7.0824e-03,\n",
       "            3.4511e-03, -2.0612e-02,  1.9700e-03,  2.2766e-02,  3.1088e-02,\n",
       "            7.8084e-03,  1.4979e-02,  8.9085e-03, -4.2074e-02, -2.5084e-02,\n",
       "            2.5190e-02,  1.0802e-02,  7.8169e-02,  7.3822e-03, -1.6420e-02,\n",
       "            1.7475e-04, -1.7904e-02, -1.2519e-02, -4.5338e-03,  3.0484e-02,\n",
       "           -1.1422e-02,  2.1765e-02, -1.9468e-03, -8.3815e-03, -7.3409e-03,\n",
       "            1.0873e-02, -1.2188e-02, -1.0292e-02,  8.7603e-03,  7.4500e-03,\n",
       "            1.7819e-03,  2.2365e-02,  1.7328e-02, -1.2085e-02,  8.4014e-03,\n",
       "            2.1003e-02,  5.0341e-03, -5.2389e-03,  1.8634e-02,  5.7822e-03,\n",
       "           -1.0785e-02,  3.9486e-02,  5.5078e-03, -7.9926e-03, -8.9409e-03,\n",
       "           -1.6955e-02, -1.5862e-02,  1.1029e-02, -1.6837e-02, -8.8895e-04,\n",
       "            2.1774e-02,  1.0489e-02, -1.5112e-02,  1.0510e-02,  8.1326e-03,\n",
       "           -1.4907e-02,  1.4861e-02, -4.6929e-03,  3.4145e-02, -1.2089e-02,\n",
       "           -1.2213e-02,  2.2625e-02, -3.7597e-03, -8.0968e-03,  4.7368e-03,\n",
       "            1.8800e-02, -9.8079e-02, -4.5514e-02, -2.0916e-02, -1.4395e-02,\n",
       "           -2.4690e-02, -2.6879e-03,  1.1432e-02, -6.5998e-03,  1.1431e-02,\n",
       "           -1.4127e-04,  2.0039e-02, -2.1528e-02, -1.9943e-02, -5.8667e-03,\n",
       "           -2.6670e-02, -7.7820e-03, -1.3542e-02, -7.0426e-03, -1.3945e-02,\n",
       "            1.1313e-02, -6.0147e-03,  1.9826e-02, -3.6162e-03,  1.1581e-02,\n",
       "           -6.5605e-03, -1.2457e-02,  1.5682e-02,  2.9204e-02,  3.3701e-03,\n",
       "           -6.8273e-03,  5.6615e-04, -1.4298e-03,  1.8995e-02,  3.9813e-02,\n",
       "            3.1168e-02,  2.4817e-02,  2.0974e-02, -4.7637e-03,  1.4747e-02,\n",
       "            2.1972e-02, -5.0200e-03,  1.1263e-02,  5.1302e-02,  5.6657e-03,\n",
       "            5.3651e-04, -2.1440e-02,  1.2138e-02, -1.0497e-02,  4.6990e-03,\n",
       "            2.7727e-02, -2.7869e-02, -5.4554e-04,  3.2653e-02,  1.5652e-02,\n",
       "            1.1317e-02, -1.0897e-02, -4.0616e-02, -3.4058e-02, -7.5577e-03,\n",
       "            6.4053e-03,  6.6166e-03, -2.4033e-02,  1.1459e-02, -1.3984e-02,\n",
       "           -8.1502e-03,  1.1163e-02,  8.2542e-03, -5.3897e-03,  2.8768e-02,\n",
       "            1.1388e-02,  1.0066e-02, -2.4167e-03,  1.8377e-02,  1.2755e-02,\n",
       "            3.8524e-02,  1.9355e-02, -1.7824e-03, -1.0501e-02, -2.3361e-02,\n",
       "            7.0351e-03,  9.4095e-03,  1.2125e-02,  1.9675e-02, -7.4414e-03,\n",
       "            7.8398e-05,  3.4587e-03, -1.0619e-02,  1.5135e-02, -4.1784e-02,\n",
       "            2.5068e-02, -6.0265e-04,  5.0474e-03, -1.8176e-02,  1.2636e-02,\n",
       "            2.9639e-03, -2.2596e-02, -1.4335e-02,  1.3512e-02, -1.1183e-02,\n",
       "            1.5760e-02,  1.0042e-03,  1.4593e-02,  3.8581e-03, -7.9545e-03,\n",
       "           -1.2658e-02, -1.6685e-02, -3.6695e-02, -1.0958e-02, -4.0801e-04,\n",
       "            2.7571e-03,  2.4460e-02, -2.5951e-03, -1.9919e-02,  9.2233e-04,\n",
       "            7.4014e-04,  2.2471e-02,  2.5441e-03,  2.8494e-02,  7.1286e-03,\n",
       "           -2.4914e-02,  1.2981e-02, -3.0747e-03, -3.3040e-02,  2.0458e-02,\n",
       "           -1.9150e-02,  2.1457e-02, -2.2732e-02,  4.8147e-03, -2.1482e-02,\n",
       "           -2.3176e-02,  1.6821e-02,  1.2087e-02,  2.5303e-02, -3.5760e-03,\n",
       "            3.3084e-02,  1.6775e-02, -1.6464e-02, -1.8466e-02,  5.6927e-03,\n",
       "           -5.8718e-02, -5.9075e-03,  2.9579e-03,  4.8263e-02,  1.8685e-02,\n",
       "            2.6903e-02, -1.6432e-02,  8.4332e-03,  4.9153e-02,  1.7822e-02,\n",
       "           -4.2425e-02,  1.8471e-02,  5.7121e-02,  1.2551e-02, -1.5298e-02,\n",
       "           -1.2078e-02,  2.3887e-02,  8.0117e-03,  5.2497e-02,  3.8526e-02,\n",
       "           -3.4434e-02, -2.6399e-02, -4.5675e-02,  3.6598e-02, -3.1015e-02,\n",
       "            1.6242e-02, -3.9804e-02, -4.9304e-03, -1.7245e-02, -2.5515e-02,\n",
       "           -1.2602e-02, -4.1680e-03,  3.8595e-02, -5.9159e-02, -7.4405e-03,\n",
       "            4.8126e-02, -2.0069e-02, -9.4412e-03, -2.6123e-02,  6.4682e-03,\n",
       "            5.1684e-03, -4.6512e-04, -4.9542e-03, -2.4016e-02,  3.6240e-02,\n",
       "           -1.5456e-02,  9.1785e-03,  1.7381e-02,  2.5564e-02, -1.5134e-02,\n",
       "           -1.9038e-02, -3.8031e-02,  6.5497e-03, -1.2592e-02, -1.2116e-02,\n",
       "            3.5311e-02,  4.4152e-02,  1.0126e-03, -4.1217e-04,  1.1888e-02,\n",
       "            1.3587e-02, -1.6456e-02,  4.7564e-02, -3.4366e-02,  4.2985e-04,\n",
       "            8.2520e-03, -4.6107e-03,  1.0151e-03, -8.5547e-03, -1.0867e-02,\n",
       "            1.1150e-02,  1.0995e-02, -6.8069e-03,  8.5477e-03,  1.4168e-02,\n",
       "           -2.4357e-04,  1.0649e-02, -1.5016e-02,  5.9280e-03,  3.4130e-03,\n",
       "            7.2499e-03, -2.3598e-02, -2.2433e-03,  1.0498e-03,  2.2947e-03,\n",
       "           -9.5744e-03, -5.1591e-03,  2.3983e-02, -8.9972e-03,  6.9834e-03,\n",
       "            4.4013e-02,  1.4017e-02,  6.8826e-03,  1.8283e-03, -5.9012e-03,\n",
       "           -1.1004e-02, -7.9089e-03,  2.0125e-03, -3.0178e-03, -6.6287e-03,\n",
       "           -1.4866e-02,  9.6949e-03,  1.2629e-02, -9.4153e-03, -1.8098e-03,\n",
       "            6.4564e-04,  5.0525e-03,  8.1944e-03, -1.4976e-02,  1.5315e-02,\n",
       "           -1.5144e-02, -3.7738e-03, -2.8949e-04,  7.7387e-03, -5.3487e-03,\n",
       "           -1.7106e-02,  1.8400e-02,  4.3561e-03,  1.9203e-02,  8.5607e-03,\n",
       "            8.2309e-03,  5.0004e-03, -1.8184e-02,  9.7382e-03,  2.7278e-04,\n",
       "            1.4275e-02,  5.5432e-03,  1.2724e-02,  1.6008e-02, -1.7855e-02,\n",
       "            1.2448e-02,  1.1468e-02, -2.9315e-03, -2.9225e-02,  4.0265e-02,\n",
       "            1.6124e-03, -7.0005e-03,  2.3734e-04,  3.2821e-03,  2.4101e-02,\n",
       "           -3.6280e-02,  2.0089e-02,  3.1981e-02,  3.9811e-03, -3.0766e-02,\n",
       "            3.8253e-03,  2.3385e-02,  8.5327e-03, -7.5271e-03,  1.5639e-03,\n",
       "            6.1634e-06, -1.1759e-02,  3.0260e-03,  1.0960e-02, -7.2339e-03,\n",
       "           -2.1711e-02,  1.7470e-02,  1.9915e-02,  2.2215e-02, -2.2295e-03,\n",
       "           -1.8607e-03,  1.7004e-02,  5.7661e-03, -5.6229e-03, -5.3306e-03,\n",
       "           -1.1477e-02, -6.7000e-04,  9.3079e-03, -2.9755e-03, -4.7655e-03,\n",
       "            3.3435e-03, -1.1307e-02,  3.9731e-03,  1.0513e-02, -3.2250e-02,\n",
       "           -9.2486e-03, -2.2905e-02,  7.3241e-03, -3.5092e-02, -1.1060e-02,\n",
       "            9.0543e-03, -4.7568e-03, -3.6439e-04, -7.6407e-04,  2.9272e-02,\n",
       "           -2.7470e-02,  1.0781e-02,  1.6108e-02,  1.9351e-02,  1.5294e-02,\n",
       "            1.4930e-02,  2.7142e-02, -1.6444e-03,  5.1867e-02, -4.1590e-02,\n",
       "            2.7485e-02, -1.8159e-02,  1.4616e-02, -2.8223e-02, -2.1836e-02,\n",
       "           -3.7035e-02, -1.2575e-02,  1.2293e-02, -5.4237e-03,  2.2001e-02,\n",
       "           -7.5271e-03, -5.5768e-02, -1.6298e-02,  3.0502e-03, -1.3728e-02,\n",
       "            3.3698e-03, -4.3384e-04, -6.9684e-03, -3.1232e-02,  4.5009e-02,\n",
       "            2.0616e-02, -1.0409e-02, -2.3090e-02, -3.4293e-02,  1.1018e-03,\n",
       "           -8.5000e-03, -2.0849e-02, -3.2696e-02,  4.0556e-02,  2.3771e-02,\n",
       "            3.9398e-02, -1.3882e-02,  5.9056e-03, -9.9360e-03,  1.5168e-02,\n",
       "           -3.1733e-02, -9.0265e-03, -3.6604e-02, -1.7821e-02,  1.9675e-02,\n",
       "            4.0934e-02, -1.2232e-02, -4.6826e-03,  7.0779e-02, -5.4972e-02,\n",
       "            1.6072e-02, -2.5883e-02, -1.1649e-03, -1.1291e-02,  1.5803e-02,\n",
       "            5.6192e-02, -2.3610e-02, -9.9647e-04, -3.2063e-02, -3.2199e-03,\n",
       "           -9.1232e-03, -2.0148e-02, -7.0093e-02, -1.2437e-01,  2.3373e-02,\n",
       "           -5.0414e-04, -2.1878e-02, -1.2511e-02, -3.4763e-02, -3.2012e-03,\n",
       "            1.2513e-02, -1.1341e-02, -4.5446e-02, -2.1909e-02,  1.0888e-02,\n",
       "            1.8216e-02,  6.7742e-03, -5.3086e-03, -2.8458e-02, -2.0761e-02,\n",
       "            3.3507e-03,  4.7860e-03,  8.0991e-03, -1.3183e-02,  2.3937e-03,\n",
       "           -1.5038e-03, -1.7413e-02,  1.8932e-03,  8.7939e-03,  8.2045e-03,\n",
       "           -3.2781e-04,  8.4948e-03, -1.5060e-02,  1.7544e-02,  1.6740e-02,\n",
       "           -3.9248e-03,  2.3059e-03,  1.2013e-02,  7.5180e-03, -4.2036e-02,\n",
       "            8.0001e-04,  1.8606e-02, -4.2438e-03,  8.7675e-03, -2.2118e-02,\n",
       "            1.1333e-03, -9.4967e-03,  1.8470e-02,  1.2243e-02, -7.8087e-03,\n",
       "            7.0338e-03,  1.5369e-02, -2.7767e-03,  1.7380e-02, -1.3810e-02,\n",
       "           -1.8087e-02, -2.5262e-04,  2.8160e-03, -1.0571e-02, -1.2355e-02,\n",
       "           -4.7663e-04, -1.0160e-03,  7.8082e-03,  9.1927e-02,  1.7398e-02,\n",
       "           -4.6346e-03, -7.7768e-03, -1.5489e-02,  6.7890e-03,  1.2695e-02,\n",
       "           -2.2040e-02,  1.0588e-03, -9.3757e-03,  1.0971e-02,  1.1449e-02,\n",
       "           -1.2551e-02,  1.2195e-02, -6.5241e-03,  3.3563e-03, -5.3109e-03,\n",
       "           -7.4463e-03, -2.6397e-03,  7.1599e-03, -1.4743e-02,  2.9362e-03,\n",
       "            5.0424e-03,  4.7287e-03, -4.4464e-03,  1.3988e-02, -2.1329e-04,\n",
       "            6.0693e-03,  3.1315e-02,  2.7540e-02, -1.9092e-02, -1.0151e-02,\n",
       "           -2.2125e-03, -1.5175e-02,  1.7594e-02, -1.3841e-03,  1.5670e-02,\n",
       "            4.5302e-03, -2.3162e-03, -2.3303e-03,  1.9857e-02, -7.0773e-03,\n",
       "           -8.5168e-03,  7.7492e-03, -1.1886e-02, -8.7682e-03,  6.6262e-03,\n",
       "           -7.4530e-04,  9.6382e-03, -2.1810e-02,  3.0818e-04,  1.5030e-02,\n",
       "           -1.1015e-02,  1.8579e-02, -2.0459e-02,  8.5800e-03,  1.7035e-02,\n",
       "            5.3892e-05, -3.4280e-03,  1.0407e-02, -4.1839e-03, -6.5801e-03,\n",
       "            2.4782e-02, -1.2955e-03,  5.8868e-03,  8.7253e-03,  2.5282e-03,\n",
       "           -8.6695e-03, -1.7313e-02, -6.2276e-03, -1.4733e-02,  3.7079e-02,\n",
       "           -2.1547e-04,  2.7257e-03, -5.9696e-04,  1.9285e-02, -2.0536e-02,\n",
       "            1.7143e-02,  1.4449e-02, -6.5542e-03, -2.3365e-02,  1.2347e-02,\n",
       "            8.0866e-03, -2.2223e-02,  3.1805e-03,  3.2623e-02, -1.3899e-02,\n",
       "           -4.5315e-02, -4.4333e-03, -8.2556e-04,  1.6854e-02,  5.2781e-03,\n",
       "           -2.3699e-02,  4.9956e-03,  4.7935e-03,  2.9074e-02,  8.2377e-04,\n",
       "           -1.3017e-02, -4.3768e-02,  1.0733e-02,  1.4151e-03, -1.1715e-02,\n",
       "            3.6786e-02,  3.3582e-04, -1.3081e-02, -2.4121e-03, -1.9138e-02,\n",
       "           -1.5444e-02,  4.6571e-03,  2.4032e-03,  2.0608e-02,  1.9170e-03,\n",
       "           -4.8462e-03, -4.6177e-03, -1.2237e-02, -2.1239e-02, -6.7373e-03,\n",
       "            2.1705e-03,  3.7158e-03,  6.8323e-03, -1.2950e-02,  1.8724e-02,\n",
       "            1.3274e-02,  1.7787e-02, -3.5668e-03,  2.7728e-03, -2.0766e-02,\n",
       "            2.5364e-03,  1.2686e-03, -1.5676e-02, -9.3327e-03, -5.6336e-03,\n",
       "            3.8475e-03, -1.5278e-02,  1.6453e-02]]], grad_fn=<ViewBackward>),\n",
       " (tensor([[[[-7.7980e-04, -6.5806e-04, -1.0521e-04,  1.1914e-03, -1.8009e-03,\n",
       "             -2.7502e-03, -7.8233e-04,  2.5860e-03, -2.2570e-02, -2.6881e-03,\n",
       "              2.3669e-03,  3.2777e-03,  1.7142e-03,  1.7085e-03,  4.7215e-04,\n",
       "             -4.3301e-03,  1.2966e-03,  3.8041e-03,  5.5218e-05, -2.4422e-03,\n",
       "             -2.8807e-03, -3.4955e-03, -3.4653e-03,  2.6404e-03, -5.9517e-03,\n",
       "             -3.8325e-03,  5.8708e-03, -2.6597e-03,  4.1050e-03,  2.5653e-03,\n",
       "             -3.9080e-03,  4.9746e-03, -2.3052e-03,  1.9391e-03,  6.0000e-03,\n",
       "              2.3973e-03,  7.9802e-05,  4.9902e-04, -2.0522e-03, -2.3937e-03,\n",
       "             -1.8747e-03,  3.5503e-03, -1.5680e-04,  5.1923e-04, -3.8865e-04,\n",
       "             -5.5323e-03,  2.4720e-03, -1.7647e-03,  1.8318e-03,  4.0571e-03,\n",
       "             -1.6462e-03,  1.8299e-04, -2.5929e-03, -4.7880e-03, -7.7311e-04,\n",
       "              4.5449e-03,  2.3443e-03, -3.8923e-03,  3.0964e-03, -6.2480e-03,\n",
       "              2.9857e-03,  4.5695e-03,  5.4555e-04, -7.6326e-03]],\n",
       "  \n",
       "           [[-6.1901e-03,  2.1954e-03, -2.0032e-03, -5.5015e-03,  4.7041e-03,\n",
       "              8.0262e-03, -6.5731e-04,  5.1535e-03,  6.6659e-03, -1.6829e-03,\n",
       "             -1.2338e-03, -4.1165e-03,  5.8288e-03,  6.5286e-03, -2.8862e-03,\n",
       "             -5.5435e-03, -4.1922e-03,  3.4933e-03, -2.5085e-03,  2.6705e-03,\n",
       "             -1.9673e-03,  1.4505e-03, -5.2694e-04,  3.4777e-03, -2.1290e-03,\n",
       "             -1.0827e-02,  5.7160e-04, -1.2771e-02,  1.2197e-02,  9.2011e-04,\n",
       "             -7.0105e-05, -2.5185e-03, -2.5450e-03,  1.4186e-03,  3.5430e-03,\n",
       "             -1.0059e-02, -9.0462e-03, -2.1483e-03, -7.2961e-03,  1.6446e-03,\n",
       "             -1.9546e-03,  1.6745e-03,  3.0747e-05,  6.8765e-04,  4.6925e-05,\n",
       "             -4.6175e-03,  1.4915e-03, -1.8102e-03,  5.8387e-03,  2.6187e-03,\n",
       "              8.0551e-04, -3.7605e-04,  4.7826e-03,  4.5015e-03,  1.8880e-03,\n",
       "              2.0086e-03, -3.0556e-03,  1.1215e-03, -3.3826e-03, -1.6811e-03,\n",
       "              2.6084e-03, -2.3130e-03,  9.7316e-03,  4.0729e-03]],\n",
       "  \n",
       "           [[ 3.3158e-02, -3.9357e-03, -5.8574e-03, -8.3691e-03, -4.3808e-03,\n",
       "              9.6372e-04,  2.7483e-03,  9.2655e-04, -9.5048e-03,  5.4070e-03,\n",
       "              4.4320e-03, -5.7101e-03,  7.0762e-03,  3.9604e-03, -4.0571e-03,\n",
       "              5.1850e-03, -2.2990e-03, -1.6759e-03,  3.5689e-03, -8.1071e-04,\n",
       "             -8.3714e-03,  2.4655e-06,  8.8419e-03, -4.9797e-03,  2.9355e-03,\n",
       "              3.7562e-03,  2.3886e-03, -1.3101e-02,  2.7496e-03, -3.6806e-03,\n",
       "              5.4301e-04,  3.3448e-04, -3.5470e-03, -3.4434e-04,  2.0638e-02,\n",
       "              6.3679e-03,  4.3933e-03, -7.7175e-03,  2.5263e-03,  6.5277e-03,\n",
       "              1.0303e-04,  9.2934e-03,  4.5492e-03, -3.1950e-03,  5.3094e-03,\n",
       "             -8.3932e-03,  1.7044e-03, -1.6819e-03,  1.6287e-03,  2.5236e-03,\n",
       "             -8.7690e-04, -8.4405e-03,  5.1847e-03,  2.9581e-03, -8.6386e-03,\n",
       "             -4.4322e-03, -2.2902e-03,  7.4777e-03, -5.2038e-03,  1.0842e-03,\n",
       "              6.3830e-03,  4.6294e-03, -4.4676e-03, -1.4646e-02]],\n",
       "  \n",
       "           [[-2.4057e-03, -1.0346e-03, -1.0573e-03,  1.2500e-03,  2.8151e-03,\n",
       "              6.4751e-03, -3.0001e-03,  1.8604e-03,  3.2033e-03,  3.3507e-03,\n",
       "              3.8772e-03, -1.0555e-03,  3.5869e-03,  2.9678e-03,  2.2149e-03,\n",
       "             -6.3072e-05,  2.4339e-04, -1.5107e-03,  8.0292e-04,  5.0392e-03,\n",
       "             -1.8044e-03, -3.5083e-03, -4.3115e-04, -1.2534e-03,  4.0202e-03,\n",
       "             -2.8886e-03,  1.6242e-03, -1.4304e-03,  2.4730e-03,  3.2120e-04,\n",
       "              1.5405e-03,  2.3828e-03, -1.3154e-02, -9.1298e-03, -3.3335e-03,\n",
       "             -1.6285e-03, -7.3392e-03, -4.1661e-03, -7.2247e-04,  6.0622e-04,\n",
       "             -1.3110e-04, -3.1256e-03, -1.6078e-03,  8.9988e-03,  5.2480e-03,\n",
       "             -3.8470e-03,  2.5489e-03, -8.3292e-04, -9.3382e-04, -2.3466e-03,\n",
       "             -6.4202e-03, -5.4375e-03, -1.0460e-03, -8.6690e-04,  9.9264e-04,\n",
       "             -1.3279e-03, -3.3971e-03, -4.1026e-03, -2.0531e-03, -4.4115e-03,\n",
       "              4.9086e-04, -2.7030e-03,  4.8619e-03, -4.1838e-03]],\n",
       "  \n",
       "           [[-2.6691e-04,  3.2127e-03,  2.1037e-03,  5.1669e-03, -2.0005e-04,\n",
       "             -3.6263e-03, -5.4068e-03, -2.5229e-03,  6.4348e-03,  3.3690e-04,\n",
       "             -4.2479e-03,  8.0654e-04, -7.7276e-03,  9.2269e-04, -2.4067e-03,\n",
       "             -3.0091e-03,  3.1880e-03, -8.5397e-03, -2.6227e-03,  1.1948e-03,\n",
       "             -1.8824e-03, -2.7381e-03,  1.9049e-03, -9.1579e-03,  6.0436e-03,\n",
       "              2.6585e-03,  2.3298e-03, -6.6079e-03,  1.2441e-02,  3.5928e-03,\n",
       "              1.5441e-03, -5.5536e-03, -3.7747e-03,  2.9258e-03, -3.2777e-03,\n",
       "             -3.6607e-03, -2.5393e-03,  3.6387e-03,  2.5710e-03, -5.8111e-04,\n",
       "              1.0417e-02,  3.9542e-03, -5.3282e-03, -1.0404e-03,  7.7495e-03,\n",
       "             -4.3610e-03, -1.1252e-03,  5.4011e-04,  3.0494e-03,  5.4364e-03,\n",
       "             -3.0751e-03,  3.2636e-03, -1.8717e-03,  2.1970e-03, -5.7621e-04,\n",
       "              2.5957e-03,  1.4918e-03, -2.4306e-03, -1.9329e-03, -3.2307e-03,\n",
       "             -1.1912e-02,  8.8809e-03,  2.5453e-03, -1.9595e-03]],\n",
       "  \n",
       "           [[-6.3161e-03, -1.3533e-02, -1.4055e-02, -5.8536e-03, -5.1156e-03,\n",
       "             -9.1114e-05,  6.3963e-03,  1.4257e-02, -6.1021e-04,  2.2210e-03,\n",
       "              5.9605e-03,  2.2709e-04,  6.7200e-03, -5.4176e-03, -2.1599e-03,\n",
       "             -3.0826e-03, -3.7544e-03,  1.6585e-03, -2.3840e-03, -1.1991e-03,\n",
       "             -1.1667e-03, -5.3543e-03, -4.0198e-04,  5.4359e-03, -3.5473e-03,\n",
       "             -4.3826e-03,  6.3052e-03,  5.3327e-03, -1.7406e-03, -1.3597e-02,\n",
       "              9.1391e-04,  3.7985e-03, -3.9346e-03, -2.1155e-03,  8.7951e-03,\n",
       "             -1.1909e-02,  5.5996e-03, -3.5501e-03,  1.5334e-02,  6.5179e-03,\n",
       "             -3.9729e-03, -7.4206e-03,  7.4139e-03, -1.9055e-03,  2.3612e-03,\n",
       "             -1.1238e-02, -2.9476e-03, -1.7415e-02, -1.7678e-03, -1.1767e-03,\n",
       "             -2.2893e-03, -4.6260e-03,  5.0488e-03,  1.9066e-04, -4.3138e-03,\n",
       "             -9.5225e-04, -5.9966e-04,  7.7655e-04,  4.9846e-03, -4.6708e-04,\n",
       "              5.4228e-03,  8.7103e-03, -4.8813e-03,  3.7611e-03]],\n",
       "  \n",
       "           [[ 2.1191e-03, -6.8024e-03, -2.7951e-03,  3.1339e-03, -9.4608e-05,\n",
       "             -1.0741e-03,  5.4122e-03,  9.2780e-03,  2.5490e-03,  4.9811e-03,\n",
       "              7.9523e-03,  3.1308e-03, -3.8672e-03, -3.1193e-03,  5.7576e-04,\n",
       "              2.2750e-03,  3.5901e-03, -1.6208e-03,  8.4059e-04, -7.0913e-04,\n",
       "             -1.6487e-03, -3.3045e-03, -7.6376e-04, -3.9418e-04, -3.9272e-04,\n",
       "              4.0530e-03,  2.5568e-03, -8.0097e-03, -5.9956e-04,  1.4689e-03,\n",
       "             -4.9968e-03, -4.1456e-03,  1.7927e-03, -1.9524e-03, -2.9090e-03,\n",
       "             -1.7050e-03, -9.6934e-04, -4.9693e-03, -2.8804e-03,  2.8753e-03,\n",
       "              1.8471e-03, -3.5075e-03, -3.2588e-03,  3.5774e-03, -1.5532e-03,\n",
       "              4.1343e-03, -2.8292e-04,  8.5009e-03,  7.4510e-03, -5.7594e-03,\n",
       "             -1.9755e-03, -1.7901e-03, -8.2944e-04, -1.7002e-03, -3.0182e-03,\n",
       "             -9.5820e-04, -7.4499e-04, -2.5865e-03,  3.4379e-04, -3.4571e-03,\n",
       "              2.8764e-03,  1.0027e-03, -1.4246e-04, -3.7130e-03]],\n",
       "  \n",
       "           [[-1.4341e-03,  1.8129e-03,  1.8307e-03, -4.8006e-03,  1.9188e-03,\n",
       "              1.1575e-02, -5.8070e-03, -1.0208e-02,  1.2869e-02,  2.2097e-04,\n",
       "             -9.7891e-04,  1.7213e-03,  2.0133e-03,  6.4054e-03,  6.5047e-04,\n",
       "              1.9842e-03,  6.1641e-03, -6.0819e-03, -5.5761e-03,  5.0444e-03,\n",
       "              3.0619e-03,  2.5571e-03,  3.7050e-03, -5.4695e-03,  4.3742e-04,\n",
       "              3.5363e-03, -3.4058e-03, -4.0434e-03, -1.8076e-02,  4.2477e-03,\n",
       "             -1.8634e-03, -6.6455e-03, -2.7215e-03, -5.4903e-03, -1.2667e-03,\n",
       "              7.8642e-03,  2.2318e-03,  4.2974e-04,  4.3467e-03,  2.2900e-03,\n",
       "             -1.2665e-03, -6.3440e-03, -2.4504e-03, -8.1339e-03, -6.3484e-03,\n",
       "              2.1217e-04, -3.9087e-04, -2.7943e-03,  3.5598e-03, -3.3474e-04,\n",
       "              5.6040e-03,  9.1944e-03, -2.2236e-03, -1.8260e-03,  1.2461e-03,\n",
       "             -1.7190e-03, -9.4363e-04,  4.5357e-04, -5.1931e-03,  1.9887e-03,\n",
       "             -9.6306e-04,  2.2765e-03, -2.0058e-02,  4.2592e-03]],\n",
       "  \n",
       "           [[ 1.7020e-03,  3.1692e-03,  7.1921e-04, -2.8597e-03, -6.8326e-03,\n",
       "             -2.6993e-03, -9.3574e-03,  6.2351e-03, -7.3901e-04,  5.7935e-03,\n",
       "             -1.0155e-03,  4.8164e-03,  2.6249e-03, -2.8891e-03, -3.3473e-03,\n",
       "             -2.9976e-03,  1.9446e-03,  1.3205e-03, -2.5491e-03, -1.9851e-03,\n",
       "              3.9177e-03,  3.7030e-03,  2.3339e-03, -2.8783e-03,  1.4636e-03,\n",
       "             -3.2535e-03, -9.5034e-03,  2.8271e-03, -9.3190e-03, -8.2312e-05,\n",
       "              2.7465e-03,  2.1135e-04, -7.4581e-03,  1.3575e-02,  2.9917e-03,\n",
       "             -6.5620e-03,  2.8324e-04,  8.0884e-03, -8.2170e-03, -4.3072e-03,\n",
       "              3.2660e-03, -1.5638e-04,  1.1665e-02,  3.5811e-03, -2.6998e-03,\n",
       "              8.6581e-03,  9.2929e-03,  9.2497e-03, -7.1043e-03, -4.5374e-04,\n",
       "              5.2602e-03,  9.8420e-04, -7.9500e-03, -8.1448e-03, -3.7335e-03,\n",
       "             -8.4095e-03,  9.2634e-03,  2.8333e-03, -3.6720e-03, -2.3433e-03,\n",
       "             -2.0668e-03, -1.2748e-02, -3.5524e-03, -2.0240e-03]],\n",
       "  \n",
       "           [[ 4.8800e-04, -9.0661e-05,  1.9597e-03,  2.5359e-03, -1.1524e-03,\n",
       "             -3.3582e-03, -3.0711e-03,  3.3554e-03,  4.8448e-03,  8.4079e-03,\n",
       "              3.9626e-03, -7.8786e-03, -7.3669e-03,  5.0285e-03,  4.0662e-03,\n",
       "              4.0981e-03, -8.1725e-03,  2.9604e-03,  5.4628e-04,  3.1695e-03,\n",
       "             -2.4278e-03,  8.9728e-05,  8.1722e-03,  1.4756e-03,  1.0930e-03,\n",
       "             -3.0006e-04, -3.2150e-04,  5.7529e-03,  5.6333e-03, -8.4237e-03,\n",
       "             -7.3909e-04, -3.0721e-03, -5.4863e-04,  6.4530e-03,  5.0608e-03,\n",
       "             -1.1604e-03,  3.7420e-03, -3.3024e-03,  1.5683e-04, -4.7267e-03,\n",
       "              3.1898e-03,  2.2888e-03, -4.3417e-03,  2.1898e-04, -7.8783e-03,\n",
       "              2.8381e-04,  2.8753e-03,  9.1599e-03,  8.3369e-03, -2.4673e-03,\n",
       "              1.9995e-03, -1.7099e-03,  8.8134e-03, -4.3299e-03, -4.0968e-03,\n",
       "              3.0996e-05,  5.8185e-03,  6.4429e-03,  8.1068e-04,  1.9685e-03,\n",
       "              2.0342e-03,  1.5221e-03,  4.4179e-03,  3.7360e-03]],\n",
       "  \n",
       "           [[-1.8440e-03,  3.3737e-03, -4.5799e-03,  1.1131e-02, -3.5976e-03,\n",
       "             -7.1801e-03,  8.4121e-04, -3.9281e-03, -3.8446e-03,  4.5133e-03,\n",
       "             -5.2526e-03,  3.4947e-03, -1.9892e-03, -4.8708e-05, -6.0389e-03,\n",
       "              3.9629e-03, -1.4668e-02,  8.8518e-03,  1.9909e-03,  5.2217e-03,\n",
       "             -6.5975e-03, -5.1146e-03, -1.4401e-03,  4.9188e-03, -7.7127e-03,\n",
       "              3.1345e-03,  4.1862e-03,  6.2971e-03, -4.5204e-03, -2.1628e-03,\n",
       "             -4.7647e-03, -4.5725e-03,  7.6832e-03, -4.1930e-03,  5.7594e-03,\n",
       "              2.0022e-03, -4.5950e-03,  4.5008e-03, -3.0466e-03,  3.1537e-04,\n",
       "              1.4641e-03,  2.9766e-02, -8.8563e-04, -3.7466e-03,  3.6527e-03,\n",
       "             -6.1827e-03, -1.3231e-03,  4.5193e-03,  2.3934e-03,  3.9589e-03,\n",
       "              3.6224e-03,  1.6189e-04, -3.7789e-03,  1.1938e-03,  3.0674e-03,\n",
       "             -6.5973e-03, -4.1290e-03,  9.0389e-03, -6.8314e-03,  4.1216e-03,\n",
       "              1.2163e-03, -2.6441e-04,  2.2211e-03, -2.0252e-03]],\n",
       "  \n",
       "           [[ 3.6908e-03, -1.3886e-03, -2.6085e-03, -5.1436e-04,  8.6401e-04,\n",
       "              3.0427e-03,  2.9934e-03, -1.2365e-02, -1.0787e-02, -8.6920e-03,\n",
       "              1.5860e-03, -1.6784e-03, -1.3433e-03, -1.1842e-03,  2.9026e-03,\n",
       "              7.1351e-03,  1.0433e-02, -3.9235e-03, -1.6798e-02, -6.1654e-03,\n",
       "             -8.9753e-03, -7.4827e-04,  5.2294e-03, -4.9602e-03,  4.1733e-04,\n",
       "              1.2334e-03, -1.0352e-02,  5.2679e-03, -3.7480e-04, -4.4957e-03,\n",
       "             -7.8214e-03,  2.0334e-03,  4.6368e-04, -7.7520e-03,  4.9632e-03,\n",
       "              4.0213e-03,  3.9320e-03,  6.0079e-03, -2.0121e-03, -2.2320e-03,\n",
       "              6.9779e-03,  6.6463e-03,  8.9440e-03, -1.9337e-03, -5.5393e-04,\n",
       "             -8.6614e-04, -1.0080e-02, -1.4497e-03, -6.3666e-03, -2.2946e-03,\n",
       "              3.6482e-04, -4.6345e-03,  4.6946e-04, -2.0178e-03, -2.7923e-03,\n",
       "             -1.1921e-02,  2.0607e-03,  3.1313e-03,  3.3156e-03,  1.0050e-02,\n",
       "              1.4181e-04, -7.5899e-03,  3.1369e-03, -1.5074e-03]]]],\n",
       "         grad_fn=<PermuteBackward>),\n",
       "  tensor([[[[ 1.5023e-02, -5.3094e-03,  2.3572e-04,  2.0522e-03, -5.7804e-03,\n",
       "              8.1983e-03, -1.1096e-02, -4.4216e-03,  1.0920e-03, -5.7501e-03,\n",
       "             -1.2049e-03,  3.3312e-03, -5.7881e-03,  3.2863e-03,  1.6009e-02,\n",
       "             -3.8654e-03,  7.0702e-03,  9.0583e-03,  9.7348e-03, -8.1410e-03,\n",
       "              1.1158e-02, -5.4021e-03, -1.4808e-02, -6.7811e-03,  5.0396e-03,\n",
       "              4.8309e-03,  1.2264e-02,  1.7033e-03,  3.8622e-03, -5.0164e-03,\n",
       "              1.0777e-02,  2.2679e-03,  1.2602e-02,  1.7358e-03, -8.8981e-03,\n",
       "             -1.6617e-03, -5.4248e-03,  1.5820e-02, -2.7107e-03,  9.3095e-03,\n",
       "              1.4642e-02,  1.8524e-03, -6.2581e-03,  1.3867e-02,  1.6465e-02,\n",
       "             -9.9101e-03, -5.9330e-03,  3.1781e-03,  6.2056e-03, -5.7156e-03,\n",
       "              1.5173e-02,  8.2161e-03, -7.5105e-03, -6.8956e-03,  1.1878e-02,\n",
       "              1.2468e-02, -1.9451e-03,  1.4344e-02,  3.4198e-03,  9.5356e-03,\n",
       "              1.9738e-03, -2.3401e-03,  2.2367e-03, -9.8477e-04]],\n",
       "  \n",
       "           [[ 2.0704e-02,  1.1214e-02,  2.2082e-02, -2.3108e-03, -2.9715e-02,\n",
       "              6.5029e-02,  2.0563e-02, -1.7909e-03,  4.6880e-02,  3.9289e-02,\n",
       "             -1.5563e-02, -1.6345e-02, -1.1725e-02,  8.4864e-03, -2.1302e-03,\n",
       "              1.5644e-03, -2.6728e-02,  4.6210e-03, -8.3575e-04,  2.1788e-02,\n",
       "             -1.8891e-02,  4.3932e-02, -3.4591e-02,  2.0594e-02,  2.4536e-02,\n",
       "             -4.4848e-02, -9.8070e-04, -2.2324e-02, -1.9449e-02,  4.4326e-02,\n",
       "             -6.9930e-02, -2.1216e-02,  1.3391e-02,  2.3917e-02,  2.3278e-02,\n",
       "              7.0228e-03,  4.4435e-02,  1.3210e-02, -5.3480e-02, -4.0530e-02,\n",
       "             -3.2905e-02, -3.2087e-02, -2.6361e-02,  1.2619e-02,  6.9959e-03,\n",
       "              3.2442e-02, -1.8067e-02, -1.1462e-02, -3.3291e-02, -3.1658e-03,\n",
       "             -7.0824e-03,  3.4511e-03, -2.0612e-02,  1.9700e-03,  2.2766e-02,\n",
       "              3.1088e-02,  7.8084e-03,  1.4979e-02,  8.9085e-03, -4.2074e-02,\n",
       "             -2.5084e-02,  2.5190e-02,  1.0802e-02,  7.8169e-02]],\n",
       "  \n",
       "           [[ 7.3822e-03, -1.6420e-02,  1.7475e-04, -1.7904e-02, -1.2519e-02,\n",
       "             -4.5338e-03,  3.0484e-02, -1.1422e-02,  2.1765e-02, -1.9468e-03,\n",
       "             -8.3815e-03, -7.3409e-03,  1.0873e-02, -1.2188e-02, -1.0292e-02,\n",
       "              8.7603e-03,  7.4500e-03,  1.7819e-03,  2.2365e-02,  1.7328e-02,\n",
       "             -1.2085e-02,  8.4014e-03,  2.1003e-02,  5.0341e-03, -5.2389e-03,\n",
       "              1.8634e-02,  5.7822e-03, -1.0785e-02,  3.9486e-02,  5.5078e-03,\n",
       "             -7.9926e-03, -8.9409e-03, -1.6955e-02, -1.5862e-02,  1.1029e-02,\n",
       "             -1.6837e-02, -8.8895e-04,  2.1774e-02,  1.0489e-02, -1.5112e-02,\n",
       "              1.0510e-02,  8.1326e-03, -1.4907e-02,  1.4861e-02, -4.6929e-03,\n",
       "              3.4145e-02, -1.2089e-02, -1.2213e-02,  2.2625e-02, -3.7597e-03,\n",
       "             -8.0968e-03,  4.7368e-03,  1.8800e-02, -9.8079e-02, -4.5514e-02,\n",
       "             -2.0916e-02, -1.4395e-02, -2.4690e-02, -2.6879e-03,  1.1432e-02,\n",
       "             -6.5998e-03,  1.1431e-02, -1.4127e-04,  2.0039e-02]],\n",
       "  \n",
       "           [[-2.1528e-02, -1.9943e-02, -5.8667e-03, -2.6670e-02, -7.7820e-03,\n",
       "             -1.3542e-02, -7.0426e-03, -1.3945e-02,  1.1313e-02, -6.0147e-03,\n",
       "              1.9826e-02, -3.6162e-03,  1.1581e-02, -6.5605e-03, -1.2457e-02,\n",
       "              1.5682e-02,  2.9204e-02,  3.3701e-03, -6.8273e-03,  5.6615e-04,\n",
       "             -1.4298e-03,  1.8995e-02,  3.9813e-02,  3.1168e-02,  2.4817e-02,\n",
       "              2.0974e-02, -4.7637e-03,  1.4747e-02,  2.1972e-02, -5.0200e-03,\n",
       "              1.1263e-02,  5.1302e-02,  5.6657e-03,  5.3651e-04, -2.1440e-02,\n",
       "              1.2138e-02, -1.0497e-02,  4.6990e-03,  2.7727e-02, -2.7869e-02,\n",
       "             -5.4554e-04,  3.2653e-02,  1.5652e-02,  1.1317e-02, -1.0897e-02,\n",
       "             -4.0616e-02, -3.4058e-02, -7.5577e-03,  6.4053e-03,  6.6166e-03,\n",
       "             -2.4033e-02,  1.1459e-02, -1.3984e-02, -8.1502e-03,  1.1163e-02,\n",
       "              8.2542e-03, -5.3897e-03,  2.8768e-02,  1.1388e-02,  1.0066e-02,\n",
       "             -2.4167e-03,  1.8377e-02,  1.2755e-02,  3.8524e-02]],\n",
       "  \n",
       "           [[ 1.9355e-02, -1.7824e-03, -1.0501e-02, -2.3361e-02,  7.0351e-03,\n",
       "              9.4095e-03,  1.2125e-02,  1.9675e-02, -7.4414e-03,  7.8398e-05,\n",
       "              3.4587e-03, -1.0619e-02,  1.5135e-02, -4.1784e-02,  2.5068e-02,\n",
       "             -6.0265e-04,  5.0474e-03, -1.8176e-02,  1.2636e-02,  2.9639e-03,\n",
       "             -2.2596e-02, -1.4335e-02,  1.3512e-02, -1.1183e-02,  1.5760e-02,\n",
       "              1.0042e-03,  1.4593e-02,  3.8581e-03, -7.9545e-03, -1.2658e-02,\n",
       "             -1.6685e-02, -3.6695e-02, -1.0958e-02, -4.0801e-04,  2.7571e-03,\n",
       "              2.4460e-02, -2.5951e-03, -1.9919e-02,  9.2233e-04,  7.4014e-04,\n",
       "              2.2471e-02,  2.5441e-03,  2.8494e-02,  7.1286e-03, -2.4914e-02,\n",
       "              1.2981e-02, -3.0747e-03, -3.3040e-02,  2.0458e-02, -1.9150e-02,\n",
       "              2.1457e-02, -2.2732e-02,  4.8147e-03, -2.1482e-02, -2.3176e-02,\n",
       "              1.6821e-02,  1.2087e-02,  2.5303e-02, -3.5760e-03,  3.3084e-02,\n",
       "              1.6775e-02, -1.6464e-02, -1.8466e-02,  5.6927e-03]],\n",
       "  \n",
       "           [[-5.8718e-02, -5.9075e-03,  2.9579e-03,  4.8263e-02,  1.8685e-02,\n",
       "              2.6903e-02, -1.6432e-02,  8.4332e-03,  4.9153e-02,  1.7822e-02,\n",
       "             -4.2425e-02,  1.8471e-02,  5.7121e-02,  1.2551e-02, -1.5298e-02,\n",
       "             -1.2078e-02,  2.3887e-02,  8.0117e-03,  5.2497e-02,  3.8526e-02,\n",
       "             -3.4434e-02, -2.6399e-02, -4.5675e-02,  3.6598e-02, -3.1015e-02,\n",
       "              1.6242e-02, -3.9804e-02, -4.9304e-03, -1.7245e-02, -2.5515e-02,\n",
       "             -1.2602e-02, -4.1680e-03,  3.8595e-02, -5.9159e-02, -7.4405e-03,\n",
       "              4.8126e-02, -2.0069e-02, -9.4412e-03, -2.6123e-02,  6.4682e-03,\n",
       "              5.1684e-03, -4.6512e-04, -4.9542e-03, -2.4016e-02,  3.6240e-02,\n",
       "             -1.5456e-02,  9.1785e-03,  1.7381e-02,  2.5564e-02, -1.5134e-02,\n",
       "             -1.9038e-02, -3.8031e-02,  6.5497e-03, -1.2592e-02, -1.2116e-02,\n",
       "              3.5311e-02,  4.4152e-02,  1.0126e-03, -4.1217e-04,  1.1888e-02,\n",
       "              1.3587e-02, -1.6456e-02,  4.7564e-02, -3.4366e-02]],\n",
       "  \n",
       "           [[ 4.2985e-04,  8.2520e-03, -4.6107e-03,  1.0151e-03, -8.5547e-03,\n",
       "             -1.0867e-02,  1.1150e-02,  1.0995e-02, -6.8069e-03,  8.5477e-03,\n",
       "              1.4168e-02, -2.4357e-04,  1.0649e-02, -1.5016e-02,  5.9280e-03,\n",
       "              3.4130e-03,  7.2499e-03, -2.3598e-02, -2.2433e-03,  1.0498e-03,\n",
       "              2.2947e-03, -9.5744e-03, -5.1591e-03,  2.3983e-02, -8.9972e-03,\n",
       "              6.9834e-03,  4.4013e-02,  1.4017e-02,  6.8826e-03,  1.8283e-03,\n",
       "             -5.9012e-03, -1.1004e-02, -7.9089e-03,  2.0125e-03, -3.0178e-03,\n",
       "             -6.6287e-03, -1.4866e-02,  9.6949e-03,  1.2629e-02, -9.4153e-03,\n",
       "             -1.8098e-03,  6.4564e-04,  5.0525e-03,  8.1944e-03, -1.4976e-02,\n",
       "              1.5315e-02, -1.5144e-02, -3.7738e-03, -2.8949e-04,  7.7387e-03,\n",
       "             -5.3487e-03, -1.7106e-02,  1.8400e-02,  4.3561e-03,  1.9203e-02,\n",
       "              8.5607e-03,  8.2309e-03,  5.0004e-03, -1.8184e-02,  9.7382e-03,\n",
       "              2.7278e-04,  1.4275e-02,  5.5432e-03,  1.2724e-02]],\n",
       "  \n",
       "           [[ 1.6008e-02, -1.7855e-02,  1.2448e-02,  1.1468e-02, -2.9315e-03,\n",
       "             -2.9225e-02,  4.0265e-02,  1.6124e-03, -7.0005e-03,  2.3734e-04,\n",
       "              3.2821e-03,  2.4101e-02, -3.6280e-02,  2.0089e-02,  3.1981e-02,\n",
       "              3.9811e-03, -3.0766e-02,  3.8253e-03,  2.3385e-02,  8.5327e-03,\n",
       "             -7.5271e-03,  1.5639e-03,  6.1634e-06, -1.1759e-02,  3.0260e-03,\n",
       "              1.0960e-02, -7.2339e-03, -2.1711e-02,  1.7470e-02,  1.9915e-02,\n",
       "              2.2215e-02, -2.2295e-03, -1.8607e-03,  1.7004e-02,  5.7661e-03,\n",
       "             -5.6229e-03, -5.3306e-03, -1.1477e-02, -6.7000e-04,  9.3079e-03,\n",
       "             -2.9755e-03, -4.7655e-03,  3.3435e-03, -1.1307e-02,  3.9731e-03,\n",
       "              1.0513e-02, -3.2250e-02, -9.2486e-03, -2.2905e-02,  7.3241e-03,\n",
       "             -3.5092e-02, -1.1060e-02,  9.0543e-03, -4.7568e-03, -3.6439e-04,\n",
       "             -7.6407e-04,  2.9272e-02, -2.7470e-02,  1.0781e-02,  1.6108e-02,\n",
       "              1.9351e-02,  1.5294e-02,  1.4930e-02,  2.7142e-02]],\n",
       "  \n",
       "           [[-1.6444e-03,  5.1867e-02, -4.1590e-02,  2.7485e-02, -1.8159e-02,\n",
       "              1.4616e-02, -2.8223e-02, -2.1836e-02, -3.7035e-02, -1.2575e-02,\n",
       "              1.2293e-02, -5.4237e-03,  2.2001e-02, -7.5271e-03, -5.5768e-02,\n",
       "             -1.6298e-02,  3.0502e-03, -1.3728e-02,  3.3698e-03, -4.3384e-04,\n",
       "             -6.9684e-03, -3.1232e-02,  4.5009e-02,  2.0616e-02, -1.0409e-02,\n",
       "             -2.3090e-02, -3.4293e-02,  1.1018e-03, -8.5000e-03, -2.0849e-02,\n",
       "             -3.2696e-02,  4.0556e-02,  2.3771e-02,  3.9398e-02, -1.3882e-02,\n",
       "              5.9056e-03, -9.9360e-03,  1.5168e-02, -3.1733e-02, -9.0265e-03,\n",
       "             -3.6604e-02, -1.7821e-02,  1.9675e-02,  4.0934e-02, -1.2232e-02,\n",
       "             -4.6826e-03,  7.0779e-02, -5.4972e-02,  1.6072e-02, -2.5883e-02,\n",
       "             -1.1649e-03, -1.1291e-02,  1.5803e-02,  5.6192e-02, -2.3610e-02,\n",
       "             -9.9647e-04, -3.2063e-02, -3.2199e-03, -9.1232e-03, -2.0148e-02,\n",
       "             -7.0093e-02, -1.2437e-01,  2.3373e-02, -5.0414e-04]],\n",
       "  \n",
       "           [[-2.1878e-02, -1.2511e-02, -3.4763e-02, -3.2012e-03,  1.2513e-02,\n",
       "             -1.1341e-02, -4.5446e-02, -2.1909e-02,  1.0888e-02,  1.8216e-02,\n",
       "              6.7742e-03, -5.3086e-03, -2.8458e-02, -2.0761e-02,  3.3507e-03,\n",
       "              4.7860e-03,  8.0991e-03, -1.3183e-02,  2.3937e-03, -1.5038e-03,\n",
       "             -1.7413e-02,  1.8932e-03,  8.7939e-03,  8.2045e-03, -3.2781e-04,\n",
       "              8.4948e-03, -1.5060e-02,  1.7544e-02,  1.6740e-02, -3.9248e-03,\n",
       "              2.3059e-03,  1.2013e-02,  7.5180e-03, -4.2036e-02,  8.0001e-04,\n",
       "              1.8606e-02, -4.2438e-03,  8.7675e-03, -2.2118e-02,  1.1333e-03,\n",
       "             -9.4967e-03,  1.8470e-02,  1.2243e-02, -7.8087e-03,  7.0338e-03,\n",
       "              1.5369e-02, -2.7767e-03,  1.7380e-02, -1.3810e-02, -1.8087e-02,\n",
       "             -2.5262e-04,  2.8160e-03, -1.0571e-02, -1.2355e-02, -4.7663e-04,\n",
       "             -1.0160e-03,  7.8082e-03,  9.1927e-02,  1.7398e-02, -4.6346e-03,\n",
       "             -7.7768e-03, -1.5489e-02,  6.7890e-03,  1.2695e-02]],\n",
       "  \n",
       "           [[-2.2040e-02,  1.0588e-03, -9.3757e-03,  1.0971e-02,  1.1449e-02,\n",
       "             -1.2551e-02,  1.2195e-02, -6.5241e-03,  3.3563e-03, -5.3109e-03,\n",
       "             -7.4463e-03, -2.6397e-03,  7.1599e-03, -1.4743e-02,  2.9362e-03,\n",
       "              5.0424e-03,  4.7287e-03, -4.4464e-03,  1.3988e-02, -2.1329e-04,\n",
       "              6.0693e-03,  3.1315e-02,  2.7540e-02, -1.9092e-02, -1.0151e-02,\n",
       "             -2.2125e-03, -1.5175e-02,  1.7594e-02, -1.3841e-03,  1.5670e-02,\n",
       "              4.5302e-03, -2.3162e-03, -2.3303e-03,  1.9857e-02, -7.0773e-03,\n",
       "             -8.5168e-03,  7.7492e-03, -1.1886e-02, -8.7682e-03,  6.6262e-03,\n",
       "             -7.4530e-04,  9.6382e-03, -2.1810e-02,  3.0818e-04,  1.5030e-02,\n",
       "             -1.1015e-02,  1.8579e-02, -2.0459e-02,  8.5800e-03,  1.7035e-02,\n",
       "              5.3892e-05, -3.4280e-03,  1.0407e-02, -4.1839e-03, -6.5801e-03,\n",
       "              2.4782e-02, -1.2955e-03,  5.8868e-03,  8.7253e-03,  2.5282e-03,\n",
       "             -8.6695e-03, -1.7313e-02, -6.2276e-03, -1.4733e-02]],\n",
       "  \n",
       "           [[ 3.7079e-02, -2.1547e-04,  2.7257e-03, -5.9696e-04,  1.9285e-02,\n",
       "             -2.0536e-02,  1.7143e-02,  1.4449e-02, -6.5542e-03, -2.3365e-02,\n",
       "              1.2347e-02,  8.0866e-03, -2.2223e-02,  3.1805e-03,  3.2623e-02,\n",
       "             -1.3899e-02, -4.5315e-02, -4.4333e-03, -8.2556e-04,  1.6854e-02,\n",
       "              5.2781e-03, -2.3699e-02,  4.9956e-03,  4.7935e-03,  2.9074e-02,\n",
       "              8.2377e-04, -1.3017e-02, -4.3768e-02,  1.0733e-02,  1.4151e-03,\n",
       "             -1.1715e-02,  3.6786e-02,  3.3582e-04, -1.3081e-02, -2.4121e-03,\n",
       "             -1.9138e-02, -1.5444e-02,  4.6571e-03,  2.4032e-03,  2.0608e-02,\n",
       "              1.9170e-03, -4.8462e-03, -4.6177e-03, -1.2237e-02, -2.1239e-02,\n",
       "             -6.7373e-03,  2.1705e-03,  3.7158e-03,  6.8323e-03, -1.2950e-02,\n",
       "              1.8724e-02,  1.3274e-02,  1.7787e-02, -3.5668e-03,  2.7728e-03,\n",
       "             -2.0766e-02,  2.5364e-03,  1.2686e-03, -1.5676e-02, -9.3327e-03,\n",
       "             -5.6336e-03,  3.8475e-03, -1.5278e-02,  1.6453e-02]]]],\n",
       "         grad_fn=<PermuteBackward>)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.bert.encoder.layer[11].attention.self(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.bert.encoder.layer[11].attention.self(zeros)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 15194, 12015,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-751c285f7dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1191\u001b[0m             \u001b[0muse_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1194\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    922\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "decoder(outputsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2280,  0.7934, -0.3273,  ..., -0.1060,  0.5652, -0.4171],\n",
       "         [ 0.7355, -0.4856,  0.2469,  ...,  0.1477,  0.7043,  0.2593],\n",
       "         [ 1.3592,  0.8465, -0.0296,  ...,  0.0119,  0.0665,  0.2345],\n",
       "         [ 0.8412,  0.5825, -0.8065,  ..., -0.0471,  0.9530, -0.4562],\n",
       "         [ 0.2643,  0.9847, -0.6103,  ...,  1.0001,  0.0587, -0.3000]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-1.5272e-01,  1.4222e-01, -2.3856e-01,  5.7524e-01, -4.6813e-01,\n",
       "         -2.1461e-01,  3.3826e-01, -1.6099e-01, -2.0227e-01,  5.3433e-01,\n",
       "          1.9483e-01, -2.2482e-01, -9.0089e-01,  8.2799e-02,  4.2339e-01,\n",
       "         -5.7091e-01,  1.7547e-01, -6.0339e-01, -5.3633e-01,  8.4052e-02,\n",
       "          6.8741e-01, -5.4140e-01,  1.9744e-01,  7.7146e-01, -9.9504e-04,\n",
       "          4.2471e-01, -5.5083e-01, -5.6025e-01,  5.4110e-01,  3.0390e-01,\n",
       "          3.9060e-02, -3.4018e-01,  6.5153e-02, -2.8344e-01, -2.9998e-02,\n",
       "         -6.2059e-01,  4.3186e-01,  4.1466e-01,  2.4212e-01,  7.0322e-01,\n",
       "          3.9196e-02, -1.6537e-02, -6.7762e-01,  3.9754e-01, -5.4139e-01,\n",
       "         -5.2647e-01, -8.5014e-01, -1.4593e-01,  1.1420e-01, -5.9844e-01,\n",
       "          2.3083e-01,  3.2402e-01,  1.6717e-02,  2.5264e-01,  1.2997e-02,\n",
       "          4.2416e-01, -5.1925e-01, -5.7911e-01, -4.1618e-01, -8.8403e-02,\n",
       "          8.0999e-01,  5.7945e-01, -3.5950e-01, -7.1264e-01, -4.3851e-01,\n",
       "         -9.9135e-02,  6.5042e-02,  5.2348e-01, -2.1090e-01, -2.8251e-02,\n",
       "         -8.8761e-01, -5.2985e-01, -8.4311e-01,  2.2448e-01, -1.1879e-02,\n",
       "         -4.5797e-02, -6.9717e-01,  4.8563e-01,  2.1370e-01,  7.1166e-01,\n",
       "         -1.6637e-01, -5.0703e-01, -3.9639e-01, -5.6714e-01, -8.5689e-02,\n",
       "         -9.7407e-01, -2.2127e-01,  5.1250e-01, -2.3176e-01, -1.6001e-01,\n",
       "         -8.2050e-02,  6.7916e-01,  2.7545e-01, -6.4596e-01,  3.4126e-01,\n",
       "          7.7718e-01,  4.4514e-01, -6.5175e-01,  5.2255e-01, -9.1990e-01,\n",
       "         -6.5923e-01, -3.1875e-01, -8.8439e-01, -3.1731e-01, -6.2310e-01,\n",
       "          7.4512e-01, -1.4925e-01,  1.0671e-01,  1.9398e-01, -5.5933e-01,\n",
       "          3.8789e-01,  8.8508e-01, -3.4201e-02, -1.2389e-01,  8.1164e-04,\n",
       "         -2.2355e-01, -1.2502e-01,  5.1311e-01,  1.2599e-01,  8.5886e-01,\n",
       "          5.8984e-01,  7.5606e-01,  2.5190e-01, -4.3122e-01,  6.8747e-01,\n",
       "         -6.9716e-02,  5.9019e-02,  1.8230e-01,  7.0896e-01, -7.0452e-02,\n",
       "         -4.5619e-01,  1.3433e-01, -6.4514e-01, -8.9252e-01, -4.5440e-01,\n",
       "         -3.5939e-01, -9.5894e-02, -1.8934e-01,  5.7337e-02,  5.9591e-01,\n",
       "          1.2496e-01, -6.2462e-01,  3.0384e-01, -3.9010e-01, -3.7742e-01,\n",
       "          5.8911e-01,  1.3635e-01,  6.5277e-01, -8.5271e-01, -1.6781e-01,\n",
       "         -6.1666e-01, -4.8084e-01, -7.1829e-01,  3.9756e-01, -8.7522e-02,\n",
       "          7.0037e-01,  3.5174e-01, -2.7383e-01, -8.5869e-02, -4.9816e-01,\n",
       "         -5.4800e-01,  2.1039e-01,  6.9660e-01, -5.3966e-01,  6.5755e-01,\n",
       "          1.4438e-01,  5.8106e-01, -6.3686e-01, -7.2030e-01, -5.1143e-01,\n",
       "          3.5532e-02, -7.3518e-01,  8.3739e-01, -6.6024e-01, -5.8655e-01,\n",
       "         -4.1520e-01,  5.1819e-01,  8.8952e-03, -3.8870e-01, -5.5584e-02,\n",
       "          3.7675e-01, -3.9783e-01, -2.0493e-01, -6.0325e-01, -4.8346e-01,\n",
       "         -8.6613e-01,  4.6769e-01, -4.4348e-01, -6.3666e-01,  2.0525e-02,\n",
       "         -3.2224e-01,  1.7667e-01,  3.7118e-01,  3.8623e-01,  5.5174e-01,\n",
       "          1.2219e-01,  7.0095e-01,  4.3123e-01, -4.9516e-01, -1.3383e-01,\n",
       "         -1.7666e-01, -9.6848e-03, -2.5809e-01, -3.4397e-01,  7.4452e-02,\n",
       "          2.0744e-03, -4.5299e-01, -3.1478e-01, -4.0820e-01, -1.6957e-01,\n",
       "          9.0135e-02, -3.1527e-01,  2.9290e-01,  1.3616e-01, -3.7972e-03,\n",
       "          2.5722e-01,  1.9763e-01, -3.2929e-01, -4.5817e-01, -6.6581e-01,\n",
       "          2.2383e-01, -9.3291e-01, -2.5766e-01, -4.2054e-01,  1.9054e-01,\n",
       "          8.2312e-02, -3.4012e-01,  8.0064e-02, -3.4175e-01,  2.8008e-01,\n",
       "         -2.5769e-01,  2.4395e-01,  8.9788e-01,  3.2562e-01, -6.4420e-01,\n",
       "          3.8920e-01,  2.3148e-01,  2.4160e-01,  4.8486e-01, -3.5031e-01,\n",
       "         -1.7654e-01, -1.5789e-01,  8.3144e-02, -4.3198e-01,  9.0436e-02,\n",
       "          3.2064e-01, -7.1898e-01,  2.8069e-01, -3.4527e-01,  1.4433e-01,\n",
       "          3.5507e-02, -2.7136e-01, -7.6599e-02,  4.5674e-01, -8.6581e-01,\n",
       "         -3.2618e-01, -8.9066e-01,  1.1187e-01, -2.4577e-01, -5.7013e-01,\n",
       "          7.9693e-01,  1.8016e-01, -1.8933e-01, -2.2474e-01, -1.1448e-01,\n",
       "          3.2560e-01, -9.0630e-01,  1.2309e-01, -5.8082e-01, -1.9068e-01,\n",
       "          3.7650e-01, -2.8226e-01,  9.5742e-01, -7.1696e-01,  4.9020e-01,\n",
       "          3.9316e-01,  4.3019e-01, -5.8911e-02,  6.9833e-01,  6.1973e-02,\n",
       "         -7.6516e-01, -1.8752e-01,  1.6585e-01, -7.0700e-01,  2.7863e-01,\n",
       "          2.8677e-01, -2.3763e-01, -1.4472e-02,  4.3273e-01, -2.1609e-02,\n",
       "          2.1777e-01,  1.8262e-02, -2.6873e-01,  8.3102e-02,  6.2987e-02,\n",
       "          2.6667e-01, -9.7602e-02,  9.5241e-02,  3.1594e-01,  8.2051e-01,\n",
       "          6.3388e-01,  2.1788e-01, -4.4948e-02, -4.7935e-01,  1.7993e-01,\n",
       "          1.0293e-01,  3.7295e-01,  4.5524e-01,  2.3265e-01,  8.4986e-01,\n",
       "         -4.6853e-02, -6.3693e-01,  5.8693e-01,  1.0603e-01, -5.1855e-01,\n",
       "         -1.7627e-01,  7.7603e-02, -5.7595e-01,  2.2843e-01, -3.2313e-02,\n",
       "         -2.0712e-02, -4.3230e-01, -4.0984e-01, -4.8569e-01, -7.3433e-01,\n",
       "          9.8511e-02, -1.4592e-01, -1.0883e-01,  2.4063e-01, -1.0449e-01,\n",
       "         -4.6019e-01, -6.3489e-01,  2.6927e-01,  1.1312e-01, -2.1261e-01,\n",
       "         -4.6905e-01, -8.1630e-01,  3.8580e-01, -1.4028e-01, -2.4426e-01,\n",
       "         -4.1781e-01,  1.7962e-01,  1.3247e-01, -3.1889e-01,  3.4064e-01,\n",
       "         -4.2238e-01, -4.1379e-01, -6.3603e-01, -3.0114e-01,  1.4984e-02,\n",
       "          3.0044e-01,  2.4410e-02,  6.0969e-01,  3.2980e-01,  3.9824e-01,\n",
       "          2.6783e-01, -2.1021e-01,  3.4915e-01,  3.6517e-01, -1.0849e-01,\n",
       "         -1.5093e-01, -3.7519e-02, -4.6669e-02, -7.2385e-01, -2.9107e-01,\n",
       "         -5.1125e-01, -3.7659e-01,  1.5581e-01, -1.0910e-01,  8.2704e-02,\n",
       "         -7.5947e-01,  3.4727e-01, -2.5319e-01,  3.0261e-01, -9.2754e-02,\n",
       "          9.1427e-01,  5.2590e-01,  1.4392e-01,  2.3623e-01,  3.8616e-01,\n",
       "         -7.6538e-01, -6.0924e-01, -8.3967e-01, -5.5030e-02,  4.8660e-01,\n",
       "          3.1318e-01, -2.2820e-01, -8.1443e-01, -6.9576e-01,  8.3838e-01,\n",
       "         -1.5078e-01,  4.3793e-01, -4.3938e-01, -1.8074e-01,  6.8036e-02,\n",
       "         -4.8889e-01, -5.8324e-01,  1.6180e-01,  6.4126e-01,  3.6749e-01,\n",
       "          3.9342e-01,  3.0062e-02, -2.7108e-01, -3.3483e-02, -7.1346e-02,\n",
       "         -3.7769e-01, -6.3567e-01,  2.9183e-01, -2.8671e-01, -3.8842e-01,\n",
       "         -4.5539e-01,  3.4512e-01, -7.3299e-01, -2.0008e-01,  1.6247e-01,\n",
       "         -4.1670e-01,  1.6518e-01,  1.0386e-01, -2.3421e-01, -3.1550e-01,\n",
       "         -5.1232e-01, -5.4569e-01, -3.1104e-01, -5.3645e-01, -1.5114e-02,\n",
       "          3.0769e-02,  1.1360e-01,  2.7137e-01,  7.0546e-01,  7.0457e-01,\n",
       "          1.3227e-01, -4.5787e-02,  3.4185e-01,  5.6524e-01,  3.7060e-01,\n",
       "          5.1363e-02, -7.0920e-02,  4.3723e-01, -2.0271e-01, -3.1369e-01,\n",
       "         -6.0132e-01, -2.5640e-01,  3.8292e-01,  5.2039e-01, -4.3837e-01,\n",
       "         -3.2809e-01,  3.6907e-01,  2.5685e-01, -1.8079e-01, -2.7413e-01,\n",
       "         -3.0154e-01, -4.5706e-01, -1.9546e-01,  7.7493e-01,  7.4163e-01,\n",
       "         -7.2440e-01,  4.1953e-01, -5.0738e-01,  6.4536e-01,  4.1524e-01,\n",
       "         -2.9731e-01,  4.3998e-01,  1.4308e-01, -3.8971e-01, -3.1193e-01,\n",
       "         -3.4492e-01, -2.6814e-01, -3.5336e-01,  3.4437e-01, -7.6527e-01,\n",
       "          6.4321e-01, -3.3882e-01,  9.1744e-01,  6.3424e-02,  1.4261e-01,\n",
       "         -5.3652e-01,  3.7805e-01,  7.1941e-01, -6.6905e-02, -2.2039e-01,\n",
       "         -7.4990e-01,  4.0188e-01,  7.1130e-01, -6.1763e-01,  6.7882e-01,\n",
       "          3.6728e-01, -7.4930e-01,  3.9081e-01, -1.9031e-01,  2.2391e-01,\n",
       "          6.1756e-01, -3.8425e-01,  1.4333e-02, -2.8492e-01, -6.1602e-02,\n",
       "         -3.2515e-01, -1.3061e-01,  5.4029e-01,  6.0166e-01,  2.4362e-01,\n",
       "          4.8080e-01,  2.4575e-01, -7.1094e-01,  1.2752e-01,  2.3859e-01,\n",
       "          1.2075e-01,  3.1776e-01, -3.3685e-01,  2.6788e-01, -2.9184e-01,\n",
       "         -2.0290e-01, -8.4541e-02, -2.9510e-01, -8.9228e-02,  2.5733e-01,\n",
       "          1.3129e-01, -5.9062e-01,  4.1130e-01, -5.8027e-01, -4.8574e-01,\n",
       "         -5.0749e-01, -4.1330e-01,  3.2769e-01,  5.8572e-01,  5.7974e-01,\n",
       "          1.6125e-01, -5.9119e-01,  1.2427e-01, -1.0428e-01, -5.3662e-02,\n",
       "          2.0334e-01, -1.9285e-01,  1.8585e-02, -7.2332e-01,  3.2404e-01,\n",
       "          8.9374e-01,  2.5093e-01,  8.5752e-01,  5.9394e-01,  6.4609e-01,\n",
       "         -6.8847e-01,  5.8318e-01, -5.3863e-01,  4.9871e-01,  3.5964e-01,\n",
       "         -2.6782e-01, -1.6501e-01,  5.3507e-01,  4.7795e-01, -8.7502e-02,\n",
       "         -5.2478e-01,  4.7519e-01, -7.5691e-01, -3.6859e-03, -7.0226e-01,\n",
       "          1.6670e-01,  2.9913e-01, -1.4486e-02,  5.6425e-01,  4.0595e-01,\n",
       "          2.4303e-01, -3.1674e-01,  5.6762e-02,  3.1777e-01, -3.6351e-01,\n",
       "         -4.3766e-01,  7.8205e-01,  7.3952e-01, -4.2517e-01, -4.2527e-01,\n",
       "         -5.2864e-01, -2.8882e-02, -1.9272e-02,  1.0670e-01,  3.5539e-01,\n",
       "          6.6611e-01,  3.4261e-01, -6.4202e-01,  1.6583e-01,  6.6195e-01,\n",
       "         -2.9254e-01, -3.2825e-01,  2.1525e-01,  6.0028e-01, -4.0505e-02,\n",
       "          8.2958e-02,  6.2560e-01,  3.0448e-02, -3.9218e-01, -1.1955e-02,\n",
       "          2.7846e-01, -2.0992e-01,  1.1385e-01,  4.7212e-01,  1.0377e-01,\n",
       "          4.4190e-02, -3.7917e-02, -1.9707e-01, -5.5964e-01,  6.8954e-01,\n",
       "          4.7094e-01, -5.0836e-01,  1.0600e-01,  6.9493e-01, -3.8598e-01,\n",
       "          8.1489e-01, -7.2481e-01, -7.5613e-03,  4.4799e-01, -5.2879e-01,\n",
       "         -5.0642e-01,  9.5398e-02, -4.5294e-03,  3.4669e-02,  1.7078e-01,\n",
       "         -2.0997e-01, -3.7068e-01, -3.1248e-01,  7.1922e-01, -5.2843e-01,\n",
       "         -2.5751e-01,  3.0158e-01,  3.7230e-01, -3.8911e-01,  2.9718e-01,\n",
       "         -1.4180e-02,  1.5706e-01,  7.4940e-01,  3.8744e-01, -3.8745e-01,\n",
       "         -6.3285e-01, -1.8847e-01,  1.1831e-02,  1.3033e-01,  5.1887e-02,\n",
       "          3.9151e-01,  3.0061e-01,  2.3166e-01,  6.4229e-01, -3.8721e-01,\n",
       "         -2.6776e-01, -7.6402e-01,  8.6213e-01,  3.8606e-01,  4.4878e-01,\n",
       "         -6.2099e-01, -6.5751e-01, -7.9400e-01, -3.3063e-01, -2.8296e-01,\n",
       "          9.1695e-01,  5.3542e-01,  2.3423e-01,  4.3117e-02, -4.5628e-01,\n",
       "         -3.2149e-01, -1.1847e-02, -7.6203e-01, -3.1315e-02, -2.5009e-01,\n",
       "         -5.8861e-01, -5.1041e-01,  6.2657e-01, -9.6071e-02, -1.9766e-01,\n",
       "          1.4701e-01,  1.7152e-01, -7.9124e-01, -4.6167e-02, -2.0936e-01,\n",
       "         -6.5997e-01,  1.3112e-02,  6.5395e-01, -5.1598e-01,  2.7478e-01,\n",
       "          6.8070e-01, -5.2875e-01,  6.5408e-02,  1.2866e-01, -1.7847e-01,\n",
       "         -1.1870e-01, -6.5895e-01, -1.9192e-01,  7.2853e-01,  1.3532e-01,\n",
       "          4.1195e-01,  4.5602e-01,  3.6557e-01, -8.1838e-03,  1.9829e-01,\n",
       "         -2.7983e-01, -1.5240e-01, -1.7644e-01,  2.7242e-01, -7.2049e-01,\n",
       "         -4.2664e-01,  1.7273e-01, -4.5768e-01,  5.8962e-01,  4.2361e-01,\n",
       "          1.2547e-01,  7.3694e-01,  2.2027e-01,  3.7709e-01, -4.8788e-01,\n",
       "         -2.1895e-01,  3.5102e-01,  4.4789e-01,  5.2618e-01,  1.4069e-01,\n",
       "          5.9329e-02,  2.1542e-01, -1.0578e-01,  9.3045e-01, -4.4523e-01,\n",
       "         -6.2709e-01, -2.9824e-01,  1.6560e-01, -3.9148e-01, -8.4668e-02,\n",
       "         -6.7712e-02,  6.9868e-01, -1.3852e-01,  1.3472e-01,  1.4908e-01,\n",
       "          3.9649e-02,  6.1810e-01,  4.2902e-01,  3.1098e-01,  4.8371e-01,\n",
       "          9.9003e-02,  3.7342e-01,  9.4637e-02, -4.8292e-01,  4.6415e-01,\n",
       "         -2.9176e-01,  3.4876e-01,  2.9716e-01, -4.9755e-01,  7.4490e-01,\n",
       "         -9.7091e-03,  2.9795e-01, -8.0575e-01,  3.7022e-01,  5.5784e-01,\n",
       "          4.4158e-01,  3.7099e-02, -2.5709e-01,  2.4026e-01,  1.1768e-01,\n",
       "          5.0398e-02, -3.9171e-01, -6.3417e-01,  2.2946e-01,  1.6238e-01,\n",
       "          1.4449e-01, -4.0952e-02,  2.6237e-01, -7.1767e-01, -2.2119e-01,\n",
       "         -6.6254e-02, -1.2731e-01,  6.9972e-01, -5.5529e-01,  1.6684e-01,\n",
       "         -1.6780e-01, -2.9686e-01, -7.8952e-02]], grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "outputs = encoder1(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15194, 12015,     5,     2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "config_encoder = BertConfig(num_hidden_layers=2)\n",
    "config_decoder = BertConfig(num_hidden_layers=2)\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "# Initializing a Bert2Bert model from the bert-base-uncased style configurations\n",
    "model = EncoderDecoderModel(config=config)\n",
    "# Accessing the model configuration\n",
    "config_encoder = model.config.encoder\n",
    "config_decoder  = model.config.decoder\n",
    "# set decoder config to causal lm\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat them=2 NN: Attention based later\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67b1319a16a8d55d48fc73d6c08eda16d2e612bbaddba2e6ca3170581266de9e"
  },
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
