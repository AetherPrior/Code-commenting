# -*- coding: utf-8 -*-
"""nmt_with_coverage_attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mZe5C5fm7D96OYIBa-eUOr2NOOPI2CIV
"""

import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from unicodedata import normalize, category
from re import sub
from os.path import dirname
from time import time

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.lstm = tf.keras.layers.LSTM(self.enc_units, return_sequences=True, return_state=True, stateful=False)

  def call(self, x):
    x = self.embedding(x)
    # output, state_h, state_c
    output, _, state = self.lstm(x)
    return output, state

class BahdanauAttention(tf.keras.layers.Layer):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.W3 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, query, values):
    '''
      query = s_t-1
      values = h_t
      coverage = coverage attention sum vector
    '''
    num_timesteps = values.shape[1]
    # query_with_time_axis = tf.expand_dims(query, axis=1)
    # print(query.shape)
    # V(tanh(W1 s_t-1 + W2 ht + W3 c'_t))
    score = tf.expand_dims(self.V(tf.nn.tanh(self.W1(query)+self.W2(values[:,0,:]))),axis=1)    
    attention = tf.nn.softmax(score, axis=1)    
    attention_weights = tf.nn.softmax(score, axis=1)   
    # attention_weights = tf.Tensor(attention)
    coverage_vector =  tf.zeros(shape=attention_weights.shape)

    for i in range(1, num_timesteps):
        coverage_vector = tf.concat([coverage_vector, tf.expand_dims(tf.math.add(coverage_vector[:,i-1,:], attention_weights[:,i-1,:]), axis=1)], axis=1)
        score =  tf.expand_dims(self.V(tf.nn.tanh(self.W1(query) + self.W2(values[:,i,:]) + self.W3(coverage_vector[:,i,:]))), axis=1)
        attention = tf.nn.softmax(score, axis=1)
        attention_weights = tf.concat([attention_weights, attention], axis=1)
    # print(query.shape, coverage_vector.shape, attention_weights.shape)      
    # score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values) + self.W3(coverage))) # e
    # print(score.shape)
    # attention_weights = tf.nn.softmax(score, axis=1)
    # c_t+1 = \sigma_{t=0}^{t} a_t 
    # coverage = tf.math.add(coverage, attention_weights)
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)
    return context_vector, attention_weights, coverage_vector

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, max_len_inp):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    # cuDNN implementation compatible LSTM
    self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, return_state=True)
    self.fc = tf.keras.layers.Dense(vocab_size)
    self.attention = BahdanauAttention(self.dec_units)

  def call(self, x, hidden, enc_output):
    x = self.embedding(x)
    # print(f"hidden: {hidden.shape} , enc_outputs: {enc_output.shape}, coverage = {self.coverage.shape}")
    context_vector, attention_weights, coverage_vector = self.attention(hidden, enc_output)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
    output, _, state = self.lstm(x)
    output = tf.reshape(output, (-1, output.shape[2]*output.shape[1]))
    x = tf.nn.softmax(self.fc(output))
    return x, state, attention_weights, coverage_vector

path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)
path_to_file = f"{dirname(path_to_zip)}/spa-eng/spa.txt"

def unicode_to_ascii(s):
    return ''.join(c for c in normalize('NFD', s) if category(c) != 'Mn')

def preprocess_sentence(w):
    w = w.lower().strip()
    w = unicode_to_ascii(w)
    w = sub(r"([?.!,¿])", r" \1 ", w)
    w = sub(r'[" "]+', " ", w)
    w = sub(r"[^a-zA-Z?.!,¿]+", " ", w)
    w = f'<start> {w} <end>'
    return w

def create_dataset(path, num_examples):
    lines = open(path, 'r', encoding='UTF-8').read().strip().split('\n')
    word_pairs = [[preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]
    return zip(*word_pairs)

def tokenize(lang):
    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    lang_tokenizer.fit_on_texts(lang)
    tensor = lang_tokenizer.texts_to_sequences(lang)
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')
    return tensor, lang_tokenizer

def load_dataset(path, num_examples=None):
    targ_lang, inp_lang = create_dataset(path, num_examples)
    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)
    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)
    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer

num_examples = 30000
input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)
max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]

input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)

LOCAL_BATCH_SIZE = 8
embedding_dim = 256
units = 1024
BUFFER_SIZE = len(input_tensor_train)
steps_per_epoch = len(input_tensor_train) // LOCAL_BATCH_SIZE
print(steps_per_epoch)
BATCH_SIZE = LOCAL_BATCH_SIZE * strategy.num_replicas_in_sync
vocab_inp_size = len(inp_lang.word_index) + 1
vocab_tar_size = len(targ_lang.word_index) + 1
dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

with strategy.scope():
    def coverage_loss(targets, predictions, attention_weights, coverage_vector, coverage_lambda=0.5):
        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)
        covloss = tf.reduce_sum(tf.minimum(coverage_vector, attention_weights))
        return loss_object(targets, predictions) + coverage_lambda*(covloss) 
  
    encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)
    decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_len_inp=max_length_inp)
    loss_function = coverage_loss
    optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)
    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9, nesterov=True)

@tf.function
def train_step(inp, targ):
    loss = 0
    with tf.GradientTape() as tape:
        enc_output, enc_hidden = encoder(inp)
        dec_hidden = enc_hidden
        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)

        for t in range(1, targ.shape[1]):
            predictions, dec_hidden, attention_weights, coverage_vector = decoder(dec_input, dec_hidden, enc_output)
            loss += loss_function(targ[:, t], predictions,attention_weights, coverage_vector)
            dec_input = tf.expand_dims(targ[:, t], 1)

        batch_loss = (loss / int(targ.shape[1]))
        total_variables = encoder.trainable_variables + decoder.trainable_variables
        gradients = tape.gradient(loss, total_variables)
        optimizer.apply_gradients(zip(gradients, total_variables))
    return batch_loss

EPOCHS = 25
LOSSES = []

for epoch in range(EPOCHS):
    start = time()
    total_loss = 0
    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
        batch_loss_as_tensor = strategy.run(train_step, args=(inp, targ))
        batch_loss = sum([a.numpy() for a in batch_loss_as_tensor.values]) * (1./BATCH_SIZE)
        total_loss += batch_loss
        if not batch % 100:
            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss}')

    LOSSES.append(total_loss / steps_per_epoch)
    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch}')
    print(f'Time taken for 1 epoch {time() - start} sec\n')

plt.plot(LOSSES, range(1, EPOCHS+1))
plt.xlabel("epochs")
plt.ylabel("losses")
plt.title("loss vs epochs")
plt.show()

def evaluate(sentence):
    sentence = preprocess_sentence(sentence)
    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')
    inputs = tf.convert_to_tensor(inputs)

    result = ''
    enc_out, enc_hidden = encoder(inputs)
    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)

    for t in range(max_length_targ):
        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)
        predicted_id = tf.argmax(predictions[0]).numpy()
        result += f"{targ_lang.index_word[predicted_id]} "
        if targ_lang.index_word[predicted_id] == '<end>':
            break
        dec_input = tf.expand_dims([predicted_id], 0)
    
    return result, sentence

def translate(sentence):
  result, sentence = evaluate(sentence)
  print(f'Input: {sentence}')
  print(f'Predicted translation: {result}'.format())

